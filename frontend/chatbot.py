# Importando framework streamlit
import streamlit as st
from helpers.uuid import generatorUUID 
from pathlib import Path

import base64
import time

from system_prompts.master_prompt import MASTER_PROMPT
from agent_models.loading import loading_chats

def type_effect(content, message_box, delay=0.03):
    """
        Esta funci√≥n aplica el efecto de typing
        como si el modelo respondiera poco a poco
    """
    placeholder    = message_box.empty()
    displayed_text = ""

    for char in content:
        displayed_text += char
        placeholder.markdown(displayed_text)
        time.sleep(delay)

def chat_message(role, content):
    """
        Esta funci√≥n coloca los mensajes
        en la interfaz del chatbot cada vez
        que suceda una interacci√≥n
    """    
    assistant_avatar = "frontend/assets/orbe_1.png"

    if role == "assistant":
        #message_box = st.chat_message( role, avatar=assistant_avatar )
        #type_effect( content, message_box, delay=0.005 )
        st.chat_message( role, avatar=assistant_avatar ).markdown( content )
    else:
        st.chat_message( role ).markdown( content )

def settings_chatbot():
    """
        Esta funci√≥n configura todos los 
        elementos necesarios para el funcionamiento de 
        la interfaz del chatbot con streamlit
    """
    # Se agrega un t√≠tulo e icono en la pesta√±a del navegador
    st.set_page_config(page_title="FVLia", page_icon="ü§ñ")

    with open("frontend/assets/orbe_1.png", "rb") as file:
        data = base64.b64encode( file.read() ).decode( "utf-8" )

    if not st.session_state.get("presentacion_activa", False):
        st.markdown(
            f"""
            <div style="
                display: flex;
                flex-direction: column;
                align-items: center;
                justify-content: center;
                text-align: center;
            ">
                <img src="data:image/png;base64,{data}" width="160" style="margin: 0;">
                <p>
                    ¬°Hola! Soy FVLia, el asistente virtual de la Fundaci√≥n Valle del Lili. 
                    Te orientar√© en todo lo que necesites sobre nuestros servicios y atenci√≥n al cliente.
                </p>
            </div>
            """,
            unsafe_allow_html=True
        )

    # Sidebar
    with st.sidebar:

        st.title("Presentaci√≥n")
        presentacion_btn = st.button("Ver presentaci√≥n", icon="üß≠")

        # Control de estado de presentaci√≥n
        if "presentacion_activa" not in st.session_state:
            st.session_state["presentacion_activa"] = False

        if presentacion_btn:
            st.session_state["presentacion_activa"] = True
            st.rerun()

        st.title("Opciones")
        new_chat = st.button("Nuevo chat", icon="‚ú®")

        if new_chat: # Se activa cuando se da click al bot√≥n "nuevo chat"
            st.session_state["presentacion_activa"] = False  
            init_new_chat()
            st.rerun()

        st.title("Chats")

        history_chats = st.session_state["chats"]

        if len( history_chats ) != 0:
            for i, chat in enumerate( history_chats ):
                if st.button(f"Messages chat {i+1}"):
                    st.session_state["presentacion_activa"] = False  
                    st.session_state["chat_messages"] = chat["messages"]
                    st.session_state["thread_id"]     = chat["thread_id"]
                    st.rerun()


def mostrar_presentacion():
    """
    Muestra la vista de presentaci√≥n principal con secciones de documentaci√≥n del sistema.
    """
    st.markdown("# üß≠ Presentaci√≥n del Proyecto")

    # -------------------------------------------------------
    # üîπ Bot√≥n para volver al chat
    # -------------------------------------------------------
    if st.button("Volver al chat", icon="üí¨"):
        st.session_state["presentacion_activa"] = False
        st.rerun()
    
    st.markdown("""
    Esta presentaci√≥n describe la arquitectura general del asistente FVLia,
    incluyendo el flujo de orquestaci√≥n entre los componentes principales:
    el *Prompt Master*, los modelos, la base de datos vectorial, el orquestador LangChain,
    la memoria conversacional y los distintos *tools* del sistema.
    """)

    # -------------------------------------------------------
    # üîπ PROMPT MASTER
    # -------------------------------------------------------
    st.subheader("üß© PROMPT MASTER")
    with st.expander("Descripci√≥n del Prompt Master"):
        st.markdown("""
        El *Prompt Master* define el comportamiento base del asistente: tono, rol,
        restricciones y objetivos. Act√∫a como el n√∫cleo de control que contextualiza
        cada interacci√≥n antes de llamar al modelo.
        """)
        st.code(f"MASTER_PROMPT = '''{MASTER_PROMPT.strip()}'''", language="python")

    # -------------------------------------------------------
    # üîπ CONFIGURACI√ìN DE MODELOS
    # -------------------------------------------------------
    st.subheader("‚öôÔ∏è Configuraci√≥n de modelos")
    with st.expander("C√≥digo de la configuraci√≥n de modelos"):
        ruta_archivo = Path("agent_models/model_config.py")
        if ruta_archivo.exists():
            codigo_modelo = ruta_archivo.read_text(encoding="utf-8")
            st.code(codigo_modelo.strip(), language="python")
        else:
            st.warning(f"No se encontr√≥ el archivo: {ruta_archivo}")

    # -------------------------------------------------------
    # üîπ BASE DE DATOS VECTORIAL
    # -------------------------------------------------------
    st.subheader("üß† Base de datos vectorial")
    with st.expander("C√≥digo de la configuraci√≥n de la base de datos vectorial"):
        ruta_archivo = Path("agent_models/model_config.py")
        if ruta_archivo.exists():
            codigo_modelo = ruta_archivo.read_text(encoding="utf-8")
            st.code(codigo_modelo.strip(), language="python")
        else:
            st.warning(f"No se encontr√≥ el archivo: {ruta_archivo}")

    # -------------------------------------------------------
    # üîπ LANGCHAIN ORQUESTADOR
    # -------------------------------------------------------
    st.subheader("üîÑ LangChain Orquestador")
    with st.expander("C√≥digo de la configuraci√≥n del orquestador LangChain"):
        ruta_archivo = Path("agent_models/model_config.py")
        if ruta_archivo.exists():
            codigo_modelo = ruta_archivo.read_text(encoding="utf-8")
            st.code(codigo_modelo.strip(), language="python")
        else:
            st.warning(f"No se encontr√≥ el archivo: {ruta_archivo}")

    # -------------------------------------------------------
    # üîπ MEMORIA DEL CHAT Y CARGA DEL HISTORIAL
    # -------------------------------------------------------
    st.subheader("üí¨ Memoria del chat y carga del historial")
    with st.expander("C√≥digo de la configuraci√≥n de la memoria del chat y carga del historial"):
        ruta_archivo = Path("agent_models/model_config.py")
        if ruta_archivo.exists():
            codigo_modelo = ruta_archivo.read_text(encoding="utf-8")
            st.code(codigo_modelo.strip(), language="python")
        else:
            st.warning(f"No se encontr√≥ el archivo: {ruta_archivo}")

    # -------------------------------------------------------
    # üîπ TOOLS (1 a 6)
    # -------------------------------------------------------
    st.subheader("üß∞ TOOLS 1‚Äì6")
    st.markdown("""
    Los *tools* ampl√≠an la capacidad del asistente, permiti√©ndole ejecutar acciones
    o consultar sistemas externos. Cada uno cumple una funci√≥n espec√≠fica.
    """)

    with st.expander("Tool 1: get_contacts_to_schedule"):
        ruta_archivo = Path("tools/tools.py")

        if ruta_archivo.exists():
            lineas = ruta_archivo.read_text(encoding="utf-8").splitlines()
            inicio, fin = 117, 153  # rango de l√≠neas que quieres mostrar

            fragmento = "\n".join(lineas[inicio-1:fin])  # recuerda que el √≠ndice empieza en 0
            st.code(fragmento.strip(), language="python")
        else:
            st.warning(f"No se encontr√≥ el archivo: {ruta_archivo}")

        ruta_archivo = Path("tools/data/contacto.json")
        if ruta_archivo.exists():
            codigo_modelo = ruta_archivo.read_text(encoding="utf-8")
            st.code(codigo_modelo.strip(), language="json")
        else:
            st.warning(f"No se encontr√≥ el archivo: {ruta_archivo}")

        st.markdown("### Ejemplo de uso")
        st.image("frontend/imgs/tool1.png", caption="Figura 1. Arquitectura de la memoria del chat", use_container_width=True)


    with st.expander("Tool 2: get_pending_appointments"):
        ruta_archivo = Path("tools/tools.py")

        if ruta_archivo.exists():
            lineas = ruta_archivo.read_text(encoding="utf-8").splitlines()
            inicio, fin = 372, 403  # rango de l√≠neas que quieres mostrar

            fragmento = "\n".join(lineas[inicio-1:fin])  # recuerda que el √≠ndice empieza en 0
            st.code(fragmento.strip(), language="python")
        else:
            st.warning(f"No se encontr√≥ el archivo: {ruta_archivo}")

        ruta_archivo = Path("tools/data/pending_appointments.json")
        if ruta_archivo.exists():
            codigo_modelo = ruta_archivo.read_text(encoding="utf-8")
            st.code(codigo_modelo.strip(), language="json")
        else:
            st.warning(f"No se encontr√≥ el archivo: {ruta_archivo}")

        st.markdown("### Ejemplo de uso")
        st.image("frontend/imgs/tool2.png", caption="Figura 2. Citas pendientes", use_container_width=True)


    with st.expander("Tool 3: get_vaccination_programs"):
        ruta_archivo = Path("tools/tools.py")

        if ruta_archivo.exists():
            lineas = ruta_archivo.read_text(encoding="utf-8").splitlines()
            inicio, fin = 405, 444  # rango de l√≠neas que quieres mostrar

            fragmento = "\n".join(lineas[inicio-1:fin])  # recuerda que el √≠ndice empieza en 0
            st.code(fragmento.strip(), language="python")
        else:
            st.warning(f"No se encontr√≥ el archivo: {ruta_archivo}")

        ruta_archivo = Path("tools/data/vacunacion.json")
        if ruta_archivo.exists():
            codigo_modelo = ruta_archivo.read_text(encoding="utf-8")
            st.code(codigo_modelo.strip(), language="json")
        else:
            st.warning(f"No se encontr√≥ el archivo: {ruta_archivo}")

        st.markdown("### Ejemplo de uso")
        st.image("frontend/imgs/tool3.png", caption="Figura 3. Esquemas de vacunaci√≥n", use_container_width=True)


    with st.expander("Tool 4: create_pqrs"):
        ruta_archivo = Path("tools/tools.py")

        if ruta_archivo.exists():
            lineas = ruta_archivo.read_text(encoding="utf-8").splitlines()
            inicio, fin = 155, 244  # rango de l√≠neas que quieres mostrar

            fragmento = "\n".join(lineas[inicio-1:fin])  # recuerda que el √≠ndice empieza en 0
            st.code(fragmento.strip(), language="python")
        else:
            st.warning(f"No se encontr√≥ el archivo: {ruta_archivo}")

        st.markdown("### Ejemplo de uso")
        st.image("frontend/imgs/tool4.png", caption="Figura 4. Crear PQRS", use_container_width=True)


    with st.expander("Tool 5: get_pqr_status"):
        ruta_archivo = Path("tools/tools.py")

        if ruta_archivo.exists():
            lineas = ruta_archivo.read_text(encoding="utf-8").splitlines()
            inicio, fin = 246, 287  # rango de l√≠neas que quieres mostrar

            fragmento = "\n".join(lineas[inicio-1:fin])  # recuerda que el √≠ndice empieza en 0
            st.code(fragmento.strip(), language="python")
        else:
            st.warning(f"No se encontr√≥ el archivo: {ruta_archivo}")

        st.markdown("### Ejemplo de uso")
        st.image("frontend/imgs/tool5.png", caption="Figura 5. Estado de PQRS", use_container_width=True)


    with st.expander("Tool 6: get_laboratory_results"):
        ruta_archivo = Path("tools/tools.py")

        if ruta_archivo.exists():
            lineas = ruta_archivo.read_text(encoding="utf-8").splitlines()
            inicio, fin = 289, 370  # rango de l√≠neas que quieres mostrar

            fragmento = "\n".join(lineas[inicio-1:fin])  # recuerda que el √≠ndice empieza en 0
            st.code(fragmento.strip(), language="python")
        else:
            st.warning(f"No se encontr√≥ el archivo: {ruta_archivo}")

        st.markdown("### Ejemplo de uso")
        st.image("frontend/imgs/tool6_1.png", use_container_width=True)
        st.image("frontend/imgs/tool6_2.png", caption="Figura 6. Resultados de laboratorio", use_container_width=True)

    st.stop()


def states_chatbot():
    """
        Funci√≥n que inicializa estados para manejarlos
        dentro del proceso de la aplicaci√≥n.
    """
    # Estado para guardar mensajes durante la interacci√≥n
    if "chat_messages" not in st.session_state:
        st.session_state["chat_messages"] = []

    # Estado para guardar todos los chat creados
    if "chats" not in st.session_state:
        st.session_state["chats"] = []

    # Estado del ID del hilo global
    if "thread_id" not in st.session_state:
        st.session_state["thread_id"] = generatorUUID()

def init_messages_assistant():
    """
        Esta funci√≥n muestra un mensaje inicial
        del asistente cuando se ingresa por primera vez
        a la interfaz. Sin embargo, cuando se empieza a 
        interactuar entonces desaparece.
    """
    init_message = """
        ¬øC√≥mo puedo ayudarte hoy?
    """

    if len( st.session_state["chat_messages"] ) == 0:
        """ st.session_state["chat_messages"].append({
            "role"   : "assistant",
            "content": init_message,
        }) """

        chat_message( role='assistant', content=init_message )

def init_new_chat():
    """
        Esta funci√≥n ejecuta el proceso para
        iniciar un nuevo chat con el modelo LLM.

        previous_chat_messages: es un un arreglo que tiene
        un conjunto de diccionarios con las conversaciones
        entre el modelo y el usuario.
    """
    previous_chat_messages = st.session_state["chat_messages"]

    # Se revisa que al menos haya m√°s de dos mensajes en el chat 
    if len(previous_chat_messages) > 1:
        st.session_state["chats"].append({
            "messages" : previous_chat_messages,
            "thread_id": st.session_state["thread_id"]
        })
        st.session_state["chat_messages"] = []
        st.session_state["thread_id"]     = generatorUUID()

        init_messages_assistant()

# -----------------------------------------------------------------
# PROCESO PRINCIPAL
# -----------------------------------------------------------------
def init_chatbot( execute_model ):
    """
        Este proceso se ejecuta cada vez que se quiera
        interactuar con el modelo.
    """
    states_chatbot()
    loading_chats( state=st.session_state )
    settings_chatbot()

    # Si la presentaci√≥n est√° activa, se muestra y se detiene aqu√≠
    if st.session_state.get("presentacion_activa", False):
        mostrar_presentacion()
        return  # Detiene el flujo del chat

    init_messages_assistant()

    # Proceso para mostrar todo el historial de conversaciones en la interfaz del chatbot
    for message in st.session_state["chat_messages"]:
        role    = message["role"]
        content = message["content"]

        chat_message(role, content)

    # Entrada del usuario
    user_input = st.chat_input("Escribe tu consulta aqu√≠...")

    if user_input:
        # ----------------------------------------------
        # Proceso para guardar entrada usuario en el estado
        # ----------------------------------------------
        st.session_state["chat_messages"].append({
            "role"   : "user",
            "content": user_input,
        })

        chat_message( role='user', content=user_input )

        # ----------------------------------------------
        # Proceso para guardar resultado modelo en el estado
        # ----------------------------------------------
        with st.spinner("Espera un momento..."):
            response = execute_model( input=user_input, thread_id=st.session_state["thread_id"] )

        st.session_state["chat_messages"].append({
            "role"   : "assistant",
            "content": response,
        })

        chat_message( role='assistant', content=response )

